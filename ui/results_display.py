"""
Results display and model comparison components
"""

import streamlit as st
import pandas as pd
import numpy as np
from typing import Dict, Any
import plotly.express as px
import plotly.graph_objects as go

def create_results_display(results: Dict[str, Any]):
    """Display model training results"""
    st.markdown("### ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼")
    
    if not results or 'model_results' not in results:
        st.warning("í‘œì‹œí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # Performance metrics table
    create_performance_table(results['model_results'])
    
    # Performance comparison charts
    create_performance_charts(results['model_results'])
    
    # Feature importance
    if 'feature_importance' in results:
        create_feature_importance_chart(results['feature_importance'])

def create_performance_table(model_results: Dict[str, Any]):
    """Create performance comparison table"""
    st.markdown("#### ğŸ† ì„±ëŠ¥ ì§€í‘œ ë¹„êµ")
    
    performance_data = []
    for model_name, metrics in model_results.items():
        performance_data.append({
            'ëª¨ë¸': model_name,
            'ì •í™•ë„': metrics.get('accuracy', 0),
            'F1-Score': metrics.get('f1_score', 0),
            'ROC-AUC': metrics.get('roc_auc', 0),
            'ì •ë°€ë„': metrics.get('precision', 0),
            'ì¬í˜„ìœ¨': metrics.get('recall', 0),
            'CV í‰ê· ': np.mean(metrics.get('cv_scores', [0])),
            'CV í‘œì¤€í¸ì°¨': np.std(metrics.get('cv_scores', [0]))
        })
    
    df = pd.DataFrame(performance_data)
    
    # Style the dataframe
    styled_df = df.style.format({
        'ì •í™•ë„': '{:.4f}',
        'F1-Score': '{:.4f}',
        'ROC-AUC': '{:.4f}',
        'ì •ë°€ë„': '{:.4f}',
        'ì¬í˜„ìœ¨': '{:.4f}',
        'CV í‰ê· ': '{:.4f}',
        'CV í‘œì¤€í¸ì°¨': '{:.4f}'
    }).background_gradient(subset=['ì •í™•ë„', 'F1-Score', 'ROC-AUC'], cmap='RdYlGn')
    
    st.dataframe(styled_df, use_container_width=True)
    
    # Highlight best model
    best_model_idx = df['ì •í™•ë„'].idxmax()
    best_model = df.iloc[best_model_idx]['ëª¨ë¸']
    best_accuracy = df.iloc[best_model_idx]['ì •í™•ë„']
    
    st.success(f"ğŸ¯ **ìµœê³  ì„±ëŠ¥**: {best_model} (ì •í™•ë„: {best_accuracy:.4f})")

def create_performance_charts(model_results: Dict[str, Any]):
    """Create performance comparison charts"""
    st.markdown("#### ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸")
    
    # Prepare data for plotting
    models = list(model_results.keys())
    metrics = ['accuracy', 'f1_score', 'roc_auc', 'precision', 'recall']
    metric_names = ['ì •í™•ë„', 'F1-Score', 'ROC-AUC', 'ì •ë°€ë„', 'ì¬í˜„ìœ¨']
    
    # Create tabs for different chart types
    tab1, tab2, tab3 = st.tabs(["ë§‰ëŒ€ ì°¨íŠ¸", "ë ˆì´ë” ì°¨íŠ¸", "ë°•ìŠ¤ í”Œë¡¯"])
    
    with tab1:
        # Bar chart comparison
        for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
            values = [model_results[model].get(metric, 0) for model in models]
            
            fig = px.bar(
                x=models,
                y=values,
                title=f"{metric_name} ë¹„êµ",
                labels={'x': 'ëª¨ë¸', 'y': metric_name},
                color=values,
                color_continuous_scale='RdYlGn'
            )
            fig.update_layout(showlegend=False)
            st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        # Radar chart
        fig = go.Figure()
        
        for model in models:
            values = [model_results[model].get(metric, 0) for metric in metrics]
            values += [values[0]]  # Close the radar chart
            
            fig.add_trace(go.Scatterpolar(
                r=values,
                theta=metric_names + [metric_names[0]],
                fill='toself',
                name=model
            ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )),
            showlegend=True,
            title="ëª¨ë¸ ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸"
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with tab3:
        # Box plot for CV scores
        cv_data = []
        for model in models:
            cv_scores = model_results[model].get('cv_scores', [])
            for score in cv_scores:
                cv_data.append({'ëª¨ë¸': model, 'CV Score': score})
        
        if cv_data:
            cv_df = pd.DataFrame(cv_data)
            fig = px.box(cv_df, x='ëª¨ë¸', y='CV Score', title="êµì°¨ê²€ì¦ ì ìˆ˜ ë¶„í¬")
            st.plotly_chart(fig, use_container_width=True)

def create_feature_importance_chart(feature_importance: pd.DataFrame):
    """Create feature importance visualization"""
    st.markdown("#### ğŸ“Š íŠ¹ì„± ì¤‘ìš”ë„")
    
    if feature_importance.empty:
        st.info("íŠ¹ì„± ì¤‘ìš”ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # Top 20 features
    top_features = feature_importance.head(20)
    
    fig = px.bar(
        top_features,
        x='importance',
        y='feature',
        orientation='h',
        title="ìƒìœ„ 20ê°œ íŠ¹ì„± ì¤‘ìš”ë„",
        labels={'importance': 'ì¤‘ìš”ë„', 'feature': 'íŠ¹ì„±'},
        color='importance',
        color_continuous_scale='viridis'
    )
    
    fig.update_layout(height=600, yaxis={'categoryorder': 'total ascending'})
    st.plotly_chart(fig, use_container_width=True)
    
    # Show full table in expander
    with st.expander("ì „ì²´ íŠ¹ì„± ì¤‘ìš”ë„ ë³´ê¸°"):
        st.dataframe(feature_importance, use_container_width=True)

def create_model_comparison_section(config: Dict[str, Any]):
    """Create comprehensive model comparison section"""
    st.title("ğŸ”¬ ëª¨ë¸ ë¹„êµ ë¶„ì„")
    
    # Check if results are available
    if 'training_results' not in st.session_state:
        st.warning("âš ï¸ ë¨¼ì € 'ëª¨ë¸ í›ˆë ¨' í˜ì´ì§€ì—ì„œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ì„¸ìš”.")
        return
    
    results = st.session_state.training_results
    
    # Overview metrics
    st.markdown("### ğŸ“‹ ëª¨ë¸ ì„±ëŠ¥ ê°œìš”")
    
    if 'model_results' in results:
        model_results = results['model_results']
        
        # Create metric cards
        cols = st.columns(len(model_results))
        
        for i, (model_name, metrics) in enumerate(model_results.items()):
            with cols[i]:
                accuracy = metrics.get('accuracy', 0)
                f1_score = metrics.get('f1_score', 0)
                
                st.metric(
                    label=model_name,
                    value=f"{accuracy:.4f}",
                    delta=f"F1: {f1_score:.4f}"
                )
        
        # Detailed comparison
        create_results_display(results)
        
        # Model analysis
        st.markdown("### ğŸ” ìƒì„¸ ë¶„ì„")
        
        selected_models = st.multiselect(
            "ë¹„êµí•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
            list(model_results.keys()),
            default=list(model_results.keys())[:2]
        )
        
        if len(selected_models) >= 2:
            create_model_comparison_analysis(model_results, selected_models)

def create_model_comparison_analysis(model_results: Dict[str, Any], selected_models: list):
    """Create detailed model comparison analysis"""
    
    # Performance difference analysis
    st.markdown("#### ğŸ“Š ì„±ëŠ¥ ì°¨ì´ ë¶„ì„")
    
    comparison_data = []
    for model in selected_models:
        metrics = model_results[model]
        comparison_data.append({
            'ëª¨ë¸': model,
            'ì •í™•ë„': metrics.get('accuracy', 0),
            'F1-Score': metrics.get('f1_score', 0),
            'ROC-AUC': metrics.get('roc_auc', 0),
            'CV í‰ê· ': np.mean(metrics.get('cv_scores', [0])),
            'CV í‘œì¤€í¸ì°¨': np.std(metrics.get('cv_scores', [0]))
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    
    # Calculate performance differences
    if len(comparison_df) == 2:
        diff_data = {}
        for metric in ['ì •í™•ë„', 'F1-Score', 'ROC-AUC']:
            diff = comparison_df.iloc[1][metric] - comparison_df.iloc[0][metric]
            diff_data[f'{metric} ì°¨ì´'] = f"{diff:+.4f}"
        
        st.markdown("**ì„±ëŠ¥ ì°¨ì´ (ëª¨ë¸2 - ëª¨ë¸1):**")
        for metric, diff in diff_data.items():
            if float(diff) > 0:
                st.success(f"{metric}: {diff}")
            else:
                st.error(f"{metric}: {diff}")
    
    # Statistical significance test (placeholder)
    st.markdown("#### ğŸ“ˆ í†µê³„ì  ìœ ì˜ì„±")
    st.info("í†µê³„ì  ìœ ì˜ì„± ê²€ì • ê²°ê³¼ëŠ” ì¶”ê°€ êµ¬í˜„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # Recommendations
    st.markdown("#### ğŸ’¡ ëª¨ë¸ ì„ íƒ ê¶Œì¥ì‚¬í•­")
    
    best_model = comparison_df.loc[comparison_df['ì •í™•ë„'].idxmax(), 'ëª¨ë¸']
    most_stable = comparison_df.loc[comparison_df['CV í‘œì¤€í¸ì°¨'].idxmin(), 'ëª¨ë¸']
    
    recommendations = [
        f"ğŸ† **ìµœê³  ì„±ëŠ¥**: {best_model}ê°€ ê°€ì¥ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì…ë‹ˆë‹¤.",
        f"ğŸ“Š **ì•ˆì •ì„±**: {most_stable}ê°€ ê°€ì¥ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.",
        "âš–ï¸ **ê¶Œì¥**: ì„±ëŠ¥ê³¼ ì•ˆì •ì„±ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”."
    ]
    
    for rec in recommendations:
        st.markdown(rec)
